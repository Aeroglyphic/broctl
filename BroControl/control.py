# Functions to control the nodes' operations.

from collections import namedtuple
import os
import time

import execute
import util
import cmdoutput
import config
import install
import plugin
import node as node_mod

# Checks multiple nodes in parallel and returns list of tuples (node, isrunning).
# For a list of (node, bool), returns True if at least one boolean is False.
def nodeFailed(nodes):
    for (node, success) in nodes:
        if not success:
            return True
    return False

# Waits for the nodes' Bro processes to reach the given status.
# Build the Bro parameters for the given node. Include
# script for live operation if live is true.
def _makeBroParams(node, live):
    args = []

    if live and node.interface:
        try:
            # If interface name contains semicolons (to aggregate traffic from
            # multiple devices with PF_RING, the interface name can be in a
            # semicolon-delimited format, such as "p2p1;p2p2"), then we must
            # quote it to prevent shell from interpreting semicolon as command
            # separator.
            args += ["-i \"%s\"" % node.interface]
        except AttributeError:
            pass

        if config.Config.savetraces == "1":
            args += ["-w trace.pcap"]

    args += ["-U .status"]
    args += ["-p broctl"]

    if live:
        args += ["-p broctl-live"]

    if node.type == "standalone":
        args += ["-p standalone"]

    for p in config.Config.prefixes.split(":"):
        args += ["-p %s" % p]

    args += ["-p %s" % node.name]

    # The order of loaded scripts is as follows:
    # 1) local.bro gives a common set of loaded scripts for all nodes.
    # 2) The common configuration of broctl is loaded via the broctl package.
    # 3) The distribution's default settings for node configuration are loaded
    #    from either the cluster framework or standalone scripts.  This also
    #    involves loading local-<node>.bro scripts.  At this point anything
    #    in the distribution's default per-node is overridable and any
    #    identifiers in local.bro are able to be used (e.g. in defining
    #    a notice policy).
    # 4) Autogenerated broctl scripts are loaded, which may contain
    #    settings that override the previously loaded node-specific scripts.
    #    (e.g. Log::default_rotation_interval is set in manager.bro,
    #    but overrided by broctl.cfg)
    args += config.Config.sitepolicystandalone.split()
    args += ["broctl"]
    if node.type == "standalone":
        args += ["broctl/standalone"]
    else:
        args += ["base/frameworks/cluster"]
        if node.type == "manager":
            args += config.Config.sitepolicymanager.split()
        elif node.type == "proxy":
            args += ["local-proxy"]
        elif node.type == "worker":
            args += config.Config.sitepolicyworker.split()
    args += ["broctl/auto"]

    if "aux_scripts" in node.__dict__:
        args += [node.aux_scripts]

    if config.Config.broargs:
        args += [config.Config.broargs]

    return args

# Build the environment variable for the given node.
def _makeEnvParam(node):
    env = ""
    if node.type != "standalone":
        env += "CLUSTER_NODE=%s" % node.name

    vars = " ".join(["%s=%s" % (key, val) for (key, val) in sorted(node.env_vars.items())])

    if vars:
        env += " " + vars

    return env


# Attach gdb to the main Bro processes on the given nodes.
def attachGdb(nodes):
    cmdout = cmdoutput.CommandOutput()
    running = isRunning(nodes, cmdout)

    cmds = []
    cmdSuccess = True
    for (node, isrunning) in running:
        if isrunning:
            cmds += [(node, "gdb-attach", ["gdb-%s" % node.name, config.Config.bro, str(node.getPID())])]
        else:
            cmdSuccess = False

    results = execute.runHelperParallel(cmds, cmdout)
    for (node, success, output) in results:
        if success:
            cmdout.info("gdb attached on %s" % node.name)
        else:
            cmdout.error("cannot attach gdb on %s: %s" % node.name, output)
            cmdSuccess = False

    return (cmdSuccess, cmdout)


class Controller:
    def __init__(self, config, ui):
        self.config = config
        self.ui = ui

    def start(self, nodes):
        manager = []
        proxies = []
        workers = []

        for n in nodes:
            if n.type == "worker":
                workers += [n]
            elif n.type == "proxy":
                proxies += [n]
            else:
                manager += [n]

        # Start nodes. Do it in the order manager, proxies, workers.

        results1 = self._startNodes(manager)

        if nodeFailed(results1):
            return results1 + [(n, False) for n in (proxies + workers)]

        results2 = self._startNodes(proxies)

        if nodeFailed(results2):
            return results1 + results2 + [(n, False) for n in workers]

        results3 = self._startNodes(workers)

        return results1 + results2 + results3


    # Starts the given nodes.
    def _startNodes(self, nodes):
        results = []

        filtered = []
        # Ignore nodes which are still running.
        for (node, isrunning) in self.isRunning(nodes):
            if not isrunning:
                filtered += [node]
                if node.hasCrashed():
                    self.ui.info("starting %s (was crashed) ..." % node.name)
                else:
                    self.ui.info("starting %s ..." % node.name)
            else:
                self.ui.info("%s still running" % node.name)

        nodes = filtered

        # Generate crash report for any crashed nodes.
        crashed = [node for node in nodes if node.hasCrashed()]
        self._makeCrashReports(crashed)

        # Make working directories.
        dirs = [(node, node.cwd()) for node in nodes]
        nodes = []
        for (node, success) in execute.mkdirs(dirs, self.ui):
            if success:
                nodes += [node]
            else:
                self.ui.error("cannot create working directory for %s" % node.name)
                results += [(node, False)]

        # Start Bro process.
        cmds = []
        envs = []
        for node in nodes:
            pin_cpu = node.pin_cpus

            # If this node isn't using CPU pinning, then use a placeholder value
            if pin_cpu == "":
                pin_cpu = -1

            cmds += [(node, "start", [node.cwd(), str(pin_cpu)] + _makeBroParams(node, True))]
            envs += [_makeEnvParam(node)]

        nodes = []
        for (node, success, output) in execute.runHelperParallel(cmds, self.ui, envs=envs):
            if success:
                nodes += [node]
                node.setPID(int(output[0]))
            else:
                self.ui.error("cannot start %s; check output of \"diag\"" % node.name)
                results += [(node, False)]

        # Check whether processes did indeed start up.
        hanging = []
        running = []

        for (node, success) in self.waitForBros(nodes, "RUNNING", 3, True):
            if success:
                running += [node]
            else:
                hanging += [node]

        # It can happen that Bro hangs in DNS lookups at startup
        # which can take a while. At this point we already know
        # that the process has been started (waitForBro ensures that).
        # If by now there is not a TERMINATED status, we assume that it
        # is doing fine and will move on to RUNNING once DNS is done.
        for (node, success) in self.waitForBros(hanging, "TERMINATED", 0, False):
            if success:
                self.ui.error("%s terminated immediately after starting; check output with \"diag\"" % node.name)
                node.clearPID()
                results += [(node, False)]
            else:
                self.ui.info("(%s still initializing)" % node.name)
                running += [node]

        for node in running:
            self.logAction(node, "started")
            results += [(node, True)]

        return results

    def isRunning(self, nodes, setcrashed=True):

        results = []
        cmds = []

        for node in nodes:
            pid = node.getPID()
            if not pid:
                results += [(node, False)]
                continue

            cmds += [(node, "check-pid", [str(pid)])]

        for (node, success, output) in execute.runHelperParallel(cmds, self.ui):

            # If we cannot connect to the host at all, we filter it out because
            # the process might actually still be running but we can't tell.
            if output == None:
                if self.config.cron == "0":
                    self.ui.error("cannot connect to %s" % node.name)
                continue

            results += [(node, success)]

            if not success:
                if setcrashed:
                    # Grmpf. It crashed.
                    node.clearPID()
                    node.setCrashed()

        return results

    def waitForBros(self, nodes, status, timeout, ensurerunning):
        # If ensurerunning is true, process must still be running.
        if ensurerunning:
            running = self.isRunning(nodes)
        else:
            running = [(node, True) for node in nodes]

        results = []

        # Determine set of nodes still to check.
        todo = {}
        for (node, isrunning) in running:
            if isrunning:
                todo[node.name] = node
            else:
                results += [(node, False)]

        while True:
            # Determine  whether process is still running. We need to do this
            # before we get the state to avoid a race condition.
            running = self.isRunning(todo.values(), setcrashed=False)

            # Check nodes' .status file
            cmds = []
            for node in todo.values():
                cmds += [(node, "cat-file", ["%s/.status" % node.cwd()])]

            for (node, success, output) in execute.runHelperParallel(cmds, self.ui):
                if success:
                    try:
                        (stat, loc) = output[0].split()
                        if status in stat:
                            # Status reached. Cool.
                            del todo[node.name]
                            results += [(node, True)]
                    except IndexError:
                        # Something's wrong. We give up on that node.
                        del todo[node.name]
                        results += [(node, False)]

            for (node, isrunning) in running:
                if node.name in todo and not isrunning:
                    # Alright, a dead node's status will not change anymore.
                    del todo[node.name]
                    results += [(node, False)]

            if not todo:
                # All done.
                break

            # Wait a bit before we start over.
            time.sleep(1)

            # Timeout reached?
            timeout -= 1
            if timeout <= 0:
                break

            util.debug(1, "Waiting for %d node(s)..." % len(todo))

        for node in todo.values():
            # These did time-out.
            results += [(node, False)]

        if todo:
            util.debug(1, "Timeout while waiting for %d node(s)" % len(todo))

        return results

    def logAction(self, node, action):
        t = time.time()
        out = open(self.config.statslog, "a")
        print >>out, t, node, "action", action
        out.close()

    # Do a "post-terminate crash" for the given nodes.
    def _makeCrashReports(self, nodes):
        for n in nodes:
            plugin.Registry.broProcessDied(n)

        msg = "If you want to help us debug this problem, then please forward\nthis mail to reports@bro.org\n"
        cmds = []
        for node in nodes:
            cmds += [(node, "run-cmd",  [os.path.join(self.config.scriptsdir, "post-terminate"), node.cwd(),  "crash"])]

        for (node, success, output) in execute.runHelperParallel(cmds, self.ui):
            if not success:
                self.ui.error("cannot run post-terminate for %s" % node.name)
            else:
                if not util.sendMail("Crash report from %s" % node.name, msg + "\n".join(output)):
                    self.ui.error("cannot send mail")

            node.clearCrashed()

    # Stop Bro processes on nodes.
    def stop(self, nodes):
        manager = []
        proxies = []
        workers = []

        for n in nodes:
            if n.type == "worker":
                workers += [n]
            elif n.type == "proxy":
                proxies += [n]
            else:
                manager += [n]


        # Stop nodes. Do it in the order workers, proxies, manager
        # (the reverse of "start").

        results1 = self._stopNodes(workers)

        if nodeFailed(results1):
            return results1 + [(n, False) for n in (proxies + manager)]

        results2 = self._stopNodes(proxies)

        if nodeFailed(results2):
            return results1 + results2 + [(n, False) for n in manager]

        results3 = self._stopNodes(manager)

        return results1 + results2 + results3

    def _stopNodes(self, nodes):

        results = []
        running = []

        # Check for crashed nodes.
        for (node, isrunning) in self.isRunning(nodes):
            if isrunning:
                running += [node]
                self.ui.info("stopping %s ..." % node.name)
            else:
                results += [(node, True)]

                if node.hasCrashed():
                    self.ui.info("%s not running (was crashed)" % node.name)
                    self._makeCrashReports([node])
                else:
                    self.ui.info("%s not running" % node.name)

        # Helper function to stop nodes with given signal.
        def stop(nodes, signal):
            cmds = []
            for node in nodes:
                cmds += [(node, "stop", [str(node.getPID()), str(signal)])]

            return execute.runHelperParallel(cmds, self.ui)

        # Stop nodes.
        for (node, success, output) in stop(running, 15):
            if not success:
                self.ui.error("failed to send stop signal to %s" % node.name)

        if running:
            time.sleep(1)

        # Check whether they terminated.
        terminated = []
        kill = []
        for (node, success) in self.waitForBros(running, "TERMINATED", int(self.config.stoptimeout), False):
            if not success:
                # Check whether it crashed during shutdown ...
                result = self.isRunning([node])
                for (node, isrunning) in result:
                    if isrunning:
                        self.ui.info("%s did not terminate ... killing ..." % node.name)
                        kill += [node]
                    else:
                        # crashed flag is set by isRunning().
                        self.ui.info("%s crashed during shutdown" % node.name)

        if kill:
            # Kill those which did not terminate gracefully.
            stop(kill, 9)
            # Give them a bit to disappear.
            time.sleep(5)

        # Check which are still running. We check all nodes to be on the safe side
        # and give them a bit more time to finally disappear.
        timeout = 10

        todo = {}
        for node in running:
            todo[node.name] = node

        while True:

            running = self.isRunning(todo.values(), setcrashed=False)

            for (node, isrunning) in running:
                if node.name in todo and not isrunning:
                    # Alright, it's gone.
                    del todo[node.name]
                    terminated += [node]
                    results += [(node, True)]

            if not todo:
                # All done.
                break

            # Wait a bit before we start over.

            if timeout <= 0:
                break

            time.sleep(1)
            timeout -= 1

        results += [(node, False) for node in todo]

        # Do post-terminate cleanup for those which terminated gracefully.
        cleanup = [node for node in terminated if not node.hasCrashed()]

        cmds = []
        for node in cleanup:
            crashflag = ""
            if node in kill:
                crashflag = "killed"

            cmds += [(node, "run-cmd",  [os.path.join(self.config.scriptsdir, "post-terminate"), node.cwd(), crashflag])]

        for (node, success, output) in execute.runHelperParallel(cmds, self.ui):
            if not success:
                self.ui.error("cannot run post-terminate for %s" % node.name)
                self.logAction(node, "stopped (failed)")
            else:
                self.logAction(node, "stopped")

            node.clearPID()
            node.clearCrashed()

        return results


    # Output status summary for nodes.
    def status(self, nodes):
        result = []

        self.ui.info("Getting process status ...")
        all = self.isRunning(nodes)
        running = []

        cmds1 = []
        cmds2 = []
        for (node, isrunning) in all:
            if isrunning:
                running += [node]
                cmds1 += [(node, "cat-file", ["%s/.startup" % node.cwd()])]
                cmds2 += [(node, "cat-file", ["%s/.status" % node.cwd()])]

        startups = execute.runHelperParallel(cmds1, self.ui)
        statuses = execute.runHelperParallel(cmds2, self.ui)

        startups = dict([(n.name, success and util.fmttime(output[0]) or "???") for (n, success, output) in startups])
        statuses = dict([(n.name, success and output[0].split()[0].lower() or "???") for (n, success, output) in statuses])

        peers = {}
        nodes = [n for n in running if statuses[n.name] == "running"]
        self.ui.info("Getting peer status ...")
        for (node, success, args) in self._queryPeerStatus(nodes):
            if success:
                peers[node.name] = []
                for f in args[0].split():
                    keyval = f.split("=")
                    if len(keyval) > 1:
                        (key, val) = keyval
                        if key == "peer" and val != "":
                            peers[node.name] += [val]

        for (node, isrunning) in all:
            node_info = {
                'name': node.name,
                'type': node.type,
                'host': node.host,
                "status": "stopped",
                "pid": None,
                "peers": None,
                "started": None,
            }
            if isrunning:
                node_info['status'] = statuses[node.name]
            elif node.hasCrashed():
                node_info['status'] = "crashed"

            if isrunning:
                node_info["pid"] = node.getPID()

                if node.name in peers:
                    node_info["peers"] = len(peers[node.name])
                else:
                    node_info["peers"] = "???"

                node_info["started"] = startups[node.name]

            result.append(node_info)

        return result

    # Check the configuration for nodes without installing first.
    def checkConfigs(self, nodes):
        return self._doCheckConfig(nodes, False, False)

    # Prints the loaded_scripts.log for either the installed scripts
    # (if check argument is false), or the original scripts (if check arg is true)
    def listScripts(self, nodes, check):
        return self._doCheckConfig(nodes, not check, True)


    def _doCheckConfig(self, nodes, installed, list_scripts):
        results = []

        manager = self.config.manager()

        all = [(node, os.path.join(self.config.tmpdir, "check-config-%s" % node.name)) for node in nodes]

        if not os.path.exists(os.path.join(self.config.scriptsdir, "broctl-config.sh")):
            # Return a failure for one node to indicate that the command failed
            results.append((all[0][0].name, False, ["broctl-config.sh not found (try 'broctl install')"]))
            return results

        nodes = []
        for (node, cwd) in all:
            if os.path.isdir(cwd):
                if not execute.rmdir(self.config.manager(), cwd, self.ui):
                    results.append((node.name, False, ["cannot remove directory %s on manager" % cwd]))
                    continue

            if not execute.mkdir(self.config.manager(), cwd, self.ui):
                results.append((node.name, False, ["cannot create directory %s on manager" % cwd]))
                continue

            nodes += [(node, cwd)]

        cmds = []
        for (node, cwd) in nodes:

            env = _makeEnvParam(node)

            installed_policies = installed and "1" or "0"
            print_scripts = list_scripts and "1" or "0"

            install.makeLayout(cwd, self.ui, True)
            install.makeLocalNetworks(cwd, self.ui, True)
            install.makeConfig(cwd, self.ui, True)

            cmd = os.path.join(self.config.scriptsdir, "check-config") + " %s %s %s %s" % (installed_policies, print_scripts, cwd, " ".join(_makeBroParams(node, False)))
            cmd += " broctl/check"

            cmds += [((node, cwd), cmd, env, None)]

        for ((node, cwd), success, output) in execute.runLocalCmdsParallel(cmds):
            results.append((node.name, success, output))
            execute.rmdir(manager, cwd, self.ui)

        return results

    def _queryPeerStatus(self, nodes):
        running = self.isRunning(nodes)

        events = []
        for (node, isrunning) in running:
            if isrunning:
                events += [(node, "Control::peer_status_request", [], "Control::peer_status_response")]

        return execute.sendEventsParallel(events)

    def executeCmd(self, nodes, cmd):
        return execute.executeCmdsParallel([(n, cmd) for n in nodes], self.ui)

    # Clean up the working directory for nodes (flushes state).
    # If cleantmp is true, also wipes ${tmpdir}; this is done
    # even when the node is still running.
    def cleanup(self, nodes, cleantmp=False):
        cmdSuccess = True

        self.ui.info("cleaning up nodes ...")
        result = self.isRunning(nodes)
        running =    [node for (node, on) in result if on]
        notrunning = [node for (node, on) in result if not on]

        results1 = execute.rmdirs([(n, n.cwd()) for n in notrunning], self.ui)
        results2 = execute.mkdirs([(n, n.cwd()) for n in notrunning], self.ui)
        if nodeFailed(results1) or nodeFailed(results2):
            cmdSuccess = False

        for node in notrunning:
            node.clearCrashed()

        for node in running:
            self.ui.info("   %s is still running, not cleaning work directory" % node.name)

        if cleantmp:
            results3 = execute.rmdirs([(n, config.Config.tmpdir) for n in running + notrunning], self.ui)
            results4 = execute.mkdirs([(n, config.Config.tmpdir) for n in running + notrunning], self.ui)
            if nodeFailed(results3) or nodeFailed(results4):
                cmdSuccess = False

        return cmdSuccess

    # Report diagostics for node (e.g., stderr output).
    def crashDiag(self, node):
        results = []

        if not execute.isdir(node, node.cwd(), self.ui):
            results.append("No work dir found")
            return (False, results)

        (rc, output) = execute.runHelper(node, self.ui, "run-cmd",  [os.path.join(config.Config.scriptsdir, "crash-diag"), node.cwd()])
        if not rc:
            results.append("cannot run crash-diag for %s" % node.name)
            return (False, results)

        if output:
            results += output

        return (True, results)

    def capstats(self, nodes, interval):
        cmdSuccess = True

        have_cflow = self.config.cflowaddress and self.config.cflowuser and self.config.cflowpassword
        have_capstats = self.config.capstatspath

        if have_cflow:
            cflow_start = self.getCFlowStatus()
            if not cflow_start:
                cmdSuccess = False

        cs_results = []
        if have_capstats:
            for (node, error, vals) in self.getCapstatsOutput(nodes, interval):
                if str(node) == "$total":
                    cs_results += [(node, error, vals)]
                else:
                    cs_results += [("%s/%s" % (node.host, node.interface), error, vals)]

                if error:
                    cmdSuccess = False

        else:
            time.sleep(interval)

        if have_cflow:
            cflow_stop = self.getCFlowStatus()
            if not cflow_stop:
                cmdSuccess = False

        diffs = []
        if have_cflow and cflow_start and cflow_stop:
            diffs = self.calculateCFlowRate(cflow_start, cflow_stop, interval)

        results = { "capstats" : cs_results, "cflow" : diffs, "success" : cmdSuccess }
        return results

    # Gather capstats from interfaces.
    #
    # Returns a list of tuples of the form (node, error, vals) where 'error' is
    # None if we were able to get the data, or otherwise a string with an error
    # message; in case there's no error, 'vals' maps tags to their values.
    #
    # Tags are those as returned by capstats on the command-line.
    #
    # If there is more than one node, then the results will also contain
    # one "pseudo-node" of the name "$total" with the sum of all individual values.
    #
    # We do all the stuff in parallel across all nodes which is why this looks
    # a bit confusing ...
    def getCapstatsOutput(self, nodes, interval):

        results = []

        hosts = {}
        for node in nodes:
            if not node.interface:
                continue

            try:
                hosts[(node.addr, node.interface)] = node
            except AttributeError:
                continue

        cmds = []

        for (addr, interface) in hosts.keys():
            node = hosts[addr, interface]

            # If interface name contains semicolons (to aggregate traffic from
            # multiple devices with PF_RING, the interface name can be in a
            # semicolon-delimited format, such as "p2p1;p2p2"), then we must
            # quote it to prevent shell from interpreting semicolon as command
            # separator (another layer of quotes is needed because the eval
            # command is used).
            capstats = [self.config.capstatspath, "-I", str(interval), "-n", "1", "-i", "'\"%s\"'" % interface]

            cmds += [(node, "run-cmd", capstats)]

        outputs = execute.runHelperParallel(cmds, self.ui)

        totals = {}

        for (node, success, output) in outputs:

            if not success:
                if output:
                    results += [(node, "%s: capstats failed (%s)" % (node.name, output[0]), {})]
                else:
                    results += [(node, "%s: cannot execute capstats" % node.name, {})]
                continue

            if not output:
                results += [(node, "%s: no capstats output" % node.name, {})]
                continue

            fields = output[0].split()[1:]

            if not fields:
                results += [(node, "%s: unexpected capstats output: %s" % (node.name, output[0]), {})]
                continue

            vals = {}

            try:
                for field in fields:
                    (key, val) = field.split("=")
                    val = float(val)
                    vals[key] = val

                    if key in totals:
                        totals[key] += val
                    else:
                        totals[key] = val

                results += [(node, None, vals)]

            except ValueError:
                results += [(node, "%s: unexpected capstats output: %s" % (node.name, output[0]), {})]

        # Add pseudo-node for totals
        if len(nodes) > 1:
            results += [(node_mod.Node(self.config, "$total"), None, totals)]

        return results

    # Get current statistics from cFlow.
    #
    # Returns dict of the form port->(cum-pkts, cum-bytes).
    #
    # Returns None if we can't run the helper sucessfully.
    def getCFlowStatus(self):
        (success, output) = execute.runLocalCmd(os.path.join(self.config.scriptsdir, "cflow-stats"))
        if not success or not output:
            self.ui.error("failed to run cflow-stats")
            return None

        vals = {}

        for line in output:
            try:
                (port, pps, bps, pkts, bytes) = line.split()
                vals[port] = (float(pkts), float(bytes))
            except ValueError:
                # Probably an error message because we can't connect.
                self.ui.error("failed to get cFlow statistics: %s" % line)
                return None

        return vals

    # Calculates the differences between two getCFlowStatus() calls.
    # Returns a list of tuples in the same form as getCapstatsOutput() does.
    def calculateCFlowRate(self, start, stop, interval):
        diffs = [(port, stop[port][0] - start[port][0], (stop[port][1] - start[port][1])) for port in start.keys() if port in stop]

        rates = []
        for (port, pkts, bytes) in diffs:
            vals = { "kpps" : "%.1f" % (pkts / 1e3 / interval) }
            if start[port][1] >= 0:
                vals["mbps"] = "%.1f" % (bytes * 8 / 1e6 / interval)

            rates += [(port, None, vals)]

        return rates

    # Update the configuration of a running instance on the fly.
    def update(self, nodes):
        running = self.isRunning(nodes)
        zone = self.config.zoneid
        if not zone:
            zone = "NOZONE"

        cmds = []
        for (node, isrunning) in running:
            if isrunning:
                env = _makeEnvParam(node)
                env += " BRO_DNS_FAKE=1"
                args = " ".join(_makeBroParams(node, False))
                cmds += [(node.name, os.path.join(self.config.scriptsdir, "update") + " %s %s %s/tcp %s" % (util.formatBroAddr(node.addr), zone, node.getPort(), args), env, None)]
                self.ui.info("updating %s ..." % node.name)

        results = execute.runLocalCmdsParallel(cmds)

        for (tag, success, output) in results:
            if not success:
                self.ui.error("could not update %s: %s" % (tag, output))
            else:
                self.ui.info("%s: %s" % (tag, output[0]))

        return [(self.config.nodes(tag=tag)[0], success) for (tag, success, output) in results]

    # Gets disk space on all volumes relevant to broctl installation.
    # Returns a list of the form:  [ (host, diskinfo), ...]
    # where diskinfo is a list of the form DiskInfo named tuple objects (fs, total, used, avail, percent) or
    # ["FAIL", <error message>] if an error is encountered.
    def df(self, nodes):
        DiskInfo = namedtuple("DiskInfo", ("fs", "total", "used", "available", "percent"))
        dirs = ("logdir", "bindir", "helperdir", "cfgdir", "spooldir", "policydir", "libdir", "tmpdir", "staticdir", "scriptsdir")

        df = {}
        for node in nodes:
            df["%s/%s" % (node.name, node.host)] = {}

        for dir in dirs:
            path = self.config.config[dir]

            cmds = []
            for node in nodes:
                if dir == "logdir" and node.type != "manager":
                    # Don't need this on the workers/proxies.
                    continue

                cmds += [(node, "df", [path])]

            results = execute.runHelperParallel(cmds, self.ui)

            for (node, success, output) in results:
                nodehost = "%s/%s" % (node.name, node.host)
                if success:
                    if not output:
                        df[nodehost]["FAIL"] = ["FAIL", "no output from df helper"]
                        continue

                    fields = output[0].split()

                    fs = fields[0]
                    # Ignore NFS mounted volumes.
                    if ":" in fs:
                        continue

                    total = float(fields[1])
                    used = float(fields[2])
                    avail = float(fields[3])
                    perc = used * 100.0 / (used + avail)
                    df[nodehost][fs] = DiskInfo(fs, total, used, avail, perc)
                else:
                    if output:
                        msg = output[0]
                    else:
                        msg = "unknown failure"
                    df[nodehost]["FAIL"] = ["FAIL", msg]

        result = []
        for node in nodes:
            nodehost = "%s/%s" % (node.name, node.host)
            result.append((nodehost, df[nodehost].values()))

        return result

    # Returns a list of tuples of the form (node, error, vals) where 'error' is an
    # error message string, or None if there was no error.  'vals' is a list of
    # dicts which map tags to their values.  Tags are "pid", "proc", "vsize",
    # "rss", "cpu", and "cmd".
    #
    # We do all the stuff in parallel across all nodes which is why this looks
    # a bit confusing ...
    def getTopOutput(self, nodes):

        results = []
        cmds = []

        running = self.isRunning(nodes)

        # Get all the PIDs first.

        pids = {}
        parents = {}

        for (node, isrunning) in running:
            if isrunning:
                pid = node.getPID()
                pids[node.name] = [pid]
                parents[node.name] = str(pid)

                cmds += [(node, "get-childs", [str(pid)])]
            else:
                results += [(node, "not running", [{}])]
                continue

        if not cmds:
            return results

        for (node, success, output) in execute.runHelperParallel(cmds, self.ui):

            if not success:
                results += [(node, "cannot get child pids", [{}])]
                continue

            pids[node.name] += [int(line) for line in output]

        cmds = []
        hosts = {}

        # Now run top once per host.
        for node in nodes:   # Do the loop again to keep the order.
            if node.name not in pids:
                continue

            if node.host in hosts:
                continue

            hosts[node.host] = 1

            cmds += [(node, "top", [])]

        if not cmds:
            return results

        res = {}
        for (node, success, output) in execute.runHelperParallel(cmds, self.ui):
            res[node.host] = (success, output)

        # Gather results for all the nodes that are running
        for node in nodes:
            if node.name not in pids:
                continue

            success, output = res[node.host]

            if not success or not output:
                results += [(node, "cannot get top output", [{}])]
                continue

            procs = [line.split() for line in output if int(line.split()[0]) in pids[node.name]]

            if not procs:
                # It's possible that the process is no longer there.
                results += [(node, "not running", [{}])]
                continue

            vals = []

            try:
                for p in procs:
                    d = {}
                    d["pid"] = int(p[0])
                    d["proc"] = (p[0] == parents[node.name] and "parent" or "child")
                    d["vsize"] = long(float(p[1])) #May be something like 2.17684e+9
                    d["rss"] = long(float(p[2]))
                    d["cpu"] = p[3]
                    d["cmd"] = " ".join(p[4:])
                    vals += [d]
            except ValueError, err:
                results += [(node, "unexpected top output: %s" % err, [{}])]
                continue

            results += [(node, None, vals)]

        return results

    # Produce a top-like output for node's processes.
    def top(self, nodes):
        results = []
        for (node, error, vals) in self.getTopOutput(nodes):
            top_info = { "name" : node.name, "type" : node.type,
                         "host" : node.host, "pid" : None, "proc" : None,
                         "vsize" : None, "rss" : None, "cpu" : None,
                         "cmd" : None, "error" : None }
            if error:
                top_info["error"] = error
                results.append(top_info)
                continue

            for d in vals:
                top_info2 = top_info.copy()
                top_info2.update(d)
                results.append(top_info2)

        return results

    def printID(self, nodes, id):
        running = self.isRunning(nodes)

        events = []
        for (node, isrunning) in running:
            if isrunning:
                events += [(node, "Control::id_value_request", [id], "Control::id_value_response")]

        eventsresults = execute.sendEventsParallel(events)

        results = [] 
        for (node, success, args) in eventsresults:
                results.append((node.name, success, args))

        return results


    def _queryNetStats(self, nodes):
        running = self.isRunning(nodes)

        events = []
        for (node, isrunning) in running:
            if isrunning:
                events += [(node, "Control::net_stats_request", [], "Control::net_stats_response")]

        return execute.sendEventsParallel(events)

    def peerStatus(self, nodes):
        results = []
        for (node, success, args) in self._queryPeerStatus(nodes):
            if success:
                results.append((node.name, success, args[0]))
            else:
                results.append((node.name, success, args))

        return results

    def netStats(self, nodes):
        results = []
        for (node, success, args) in self._queryNetStats(nodes):
            if success:
                results.append((node.name, success, args[0].strip()))
            else:
                results.append((node.name, success, args))

        return results

    def processTrace(self, trace, bro_options, bro_scripts):
        if not os.path.isfile(trace):
            self.ui.error("trace file not found: %s" % trace)
            return False

        if not os.path.exists(os.path.join(self.config.scriptsdir, "broctl-config.sh")):
            self.ui.error("broctl-config.sh not found (try 'broctl install')")
            return False

        standalone = (self.config.standalone == "1")
        if standalone:
            tag = "standalone"
        else:
            tag = "workers"

        node = self.config.nodes(tag=tag)[0]

        cwd = os.path.join(self.config.tmpdir, "testing")

        if not execute.rmdir(self.config.manager(), cwd, self.ui):
            self.ui.error("cannot remove directory %s on manager" % cwd)
            return False

        if not execute.mkdir(self.config.manager(), cwd, self.ui):
            self.ui.error("cannot create directory %s on manager" % cwd)
            return False

        env = _makeEnvParam(node)

        bro_args =  " ".join(bro_options + _makeBroParams(node, False))
        bro_args += " broctl/process-trace"

        if bro_scripts:
            bro_args += " " + " ".join(bro_scripts)

        cmd = os.path.join(self.config.scriptsdir, "run-bro-on-trace") + " %s %s %s %s" % (0, cwd, trace, bro_args)

        self.ui.info(cmd)

        (success, output) = execute.runLocalCmd(cmd, env, donotcaptureoutput=True)

        for line in output:
            self.ui.info(line)

        self.ui.info("")
        self.ui.info("### Bro output in %s" % cwd)

        return success

