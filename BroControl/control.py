# Functions to control the nodes' operations.

from collections import namedtuple
import glob
import os
import time
import logging

from BroControl import execute
from BroControl import events
from BroControl import util
from BroControl import config
from BroControl import install
from BroControl import cron
from BroControl import node as node_mod
from BroControl import cmdresult


# Waits for the nodes' Bro processes to reach the given status.
# Build the Bro parameters for the given node. Include
# script for live operation if live is true.
def _makeBroParams(node, live):
    args = []

    if live and node.interface:
        try:
            # Interface name needs quotes so that shell doesn't interpret any
            # potential metacharacters in the name.
            args += ["-i", "'%s'" % node.interface]
        except AttributeError:
            pass

        if config.Config.savetraces == "1":
            args += ["-w", "trace.pcap"]

    args += ["-U", ".status"]
    args += ["-p", "broctl"]

    if live:
        args += ["-p", "broctl-live"]

    if node.type == "standalone":
        args += ["-p", "standalone"]

    for p in config.Config.prefixes.split(":"):
        args += ["-p", "%s" % p]

    args += ["-p", "%s" % node.name]

    # The order of loaded scripts is as follows:
    # 1) local.bro gives a common set of loaded scripts for all nodes.
    # 2) The common configuration of broctl is loaded via the broctl package.
    # 3) The distribution's default settings for node configuration are loaded
    #    from either the cluster framework or standalone scripts.  This also
    #    involves loading local-<node>.bro scripts.  At this point anything
    #    in the distribution's default per-node is overridable and any
    #    identifiers in local.bro are able to be used (e.g. in defining
    #    a notice policy).
    # 4) Autogenerated broctl scripts are loaded, which may contain
    #    settings that override the previously loaded node-specific scripts.
    #    (e.g. Log::default_rotation_interval is set in manager.bro,
    #    but overrided by broctl.cfg)
    args += config.Config.sitepolicystandalone.split()
    args += ["broctl"]
    if node.type == "standalone":
        args += ["broctl/standalone"]
    else:
        args += ["base/frameworks/cluster"]
        if node.type == "manager":
            args += config.Config.sitepolicymanager.split()
        elif node.type == "proxy":
            args += ["local-proxy"]
        elif node.type == "worker":
            args += config.Config.sitepolicyworker.split()
        elif node.type == "peer":
            # FIXME do nothing
            print "peer " + str(node.name) + ", do nothing here"
        else:
            raise RuntimeError("no configuration for node of type " + str(node.type) + " found")

    args += ["broctl/auto"]

    if getattr(node, "aux_scripts", None):
        args += [node.aux_scripts]

    if config.Config.broargs:
        # Some args in broargs might contain spaces, so we cannot split it.
        args += [config.Config.broargs]

    return args

# Build the environment variable for the given node.
def _makeEnvParam(node, returnlist=False):
    envs = []
    if node.type != "standalone" and node.type != "peer":
        envs.append("CLUSTER_NODE=%s" % node.name)

    envs += ["%s=%s" % (key, val) for (key, val) in sorted(node.env_vars.items())]

    if returnlist:
        l1 = [("-v", i) for i in envs]
        return [j for i in l1 for j in i]

    return " ".join(envs)

class Controller:
    def __init__(self, config, ui, executor, pluginregistry):
        self.config = config
        self.ui = ui
        self.executor = executor
        self.pluginregistry = pluginregistry

    def start(self, nodes):
        results = cmdresult.CmdResult()
        manager = []
        proxies = []
        workers = []
        peers = []

        for n in nodes:
            n.setExpectRunning(True)

            if n.type == "worker":
                workers += [n]
            elif n.type == "proxy":
                proxies += [n]
            elif n.type == "manager":
                manager += [n]
            elif n.type == "peer":
                peers += [n]
            elif n.type == "standalone":
                manager += [n]
            else:
                raise RuntimeError("node type " + str(n.type) + " unknown")

        if not manager:
            local= config.Config.getLocalNode()
            if not local in peers:
                raise RuntimeError("we have no manager and local node is no peer")
            else:
                manager += [local]

        # Start nodes. Do it in the order manager, proxies, workers, peers.
        logging.debug(str(self.config.getLocalNode().name) + "@" + str(self.config.localaddrs[0]) + "*** start " + str(len(manager)) + " manager, " + str(len(proxies)) + " proxies, " + str(len(workers)) + " workers, " + str(len(peers)) + " peers")

        if manager:
            self._startNodes(manager, results)

            if results.failed():
                for n in (proxies + workers + peers):
                    results.set_node_fail(n)
                return results

        if proxies:
            self._startNodes(proxies, results)

            if results.failed():
                for n in (workers + peers):
                    results.set_node_fail(n)
                return results

        if workers:
            self._startNodes(workers, results)

            if results.failed():
                for n in peers:
                    results.set_node_fail(n)
                return results

        if peers:
            self._startPeers(peers, results)

        return results

    # Starts the given nodes.
    def _startNodes(self, nodes, results):

        filtered = []
        localNode = self.config.getLocalNode()

        # Ignore nodes which are still running.
        for (node, isrunning) in self.isRunning(nodes):
            if not isrunning:
                filtered += [node]
                if node.hasCrashed():
                    self.ui.info(str(localNode.name) + " :: starting node %s (was crashed) ..." % node.name)
                else:
                    self.ui.info(str(localNode.name) + " :: starting node %s ..." % node.name)
            else:
                self.ui.info(str(localNode.name) + " :: node %s still running" % node.name)

        nodes = filtered

        # Generate crash report for any crashed nodes.
        crashed = [node for node in nodes if node.hasCrashed()]
        self._makeCrashReports(crashed)

        # Make working directories.
        dirs = [(node, node.cwd()) for node in nodes]
        nodes = []
        for (node, success) in self.executor.mkdirs(dirs):
            if success:
                nodes += [node]
            else:
                self.ui.error(str(localNode.name) + " :: cannot create working directory for %s" % node.name)
                results.set_node_fail(node)

        # Start Bro process.
        cmds = []
        for node in nodes:
            envs = []
            pin_cpu = node.pin_cpus

            # If this node isn't using CPU pinning, then use a placeholder value
            if pin_cpu == "":
                pin_cpu = -1

            envs = _makeEnvParam(node, True)
            cmds += [(node, "start", envs + [node.cwd(), str(pin_cpu)] + _makeBroParams(node, True))]

        nodes = []
        # Note: the shell is used to interpret the command because broargs
        # might contain quoted arguments.
        for (node, success, output) in self.executor.runHelperParallel(cmds, shell=True):
            if success:
                nodes += [node]
                node.setPID(int(output[0]))
            else:
                self.ui.error(str(localNode.name) + " :: cannot start %s; check output of \"diag\"" % node.name)
                results.set_node_fail(node)
                if output:
                    for line in output:
                        self.ui.error("%s" % line)

        # Check whether processes did indeed start up.
        hanging = []
        running = []

        for (node, success) in self.waitForBros(nodes, "RUNNING", 3, True):
            if success:
                running += [node]
            else:
                hanging += [node]

        # It can happen that Bro hangs in DNS lookups at startup
        # which can take a while. At this point we already know
        # that the process has been started (waitForBro ensures that).
        # If by now there is not a TERMINATED status, we assume that it
        # is doing fine and will move on to RUNNING once DNS is done.
        for (node, success) in self.waitForBros(hanging, "TERMINATED", 0, False):
            if success:
                self.ui.error("%s terminated immediately after starting; check output with \"diag\"" % node.name)
                node.clearPID()
                results.set_node_fail(node)
            else:
                self.ui.info("(%s still initializing)" % node.name)
                running += [node]

        for node in running:
            self.logAction(node, "started")
            results.set_node_success(node)

        return results

    # Start peers by copying all configuration files
    # and initiating broctl on them
    def _startPeers(self, peers, results):
        localNode = self.config.getLocalNode()

        if localNode in peers:
            logging.debug("something is wrong with the peer list@" + str(localNode.name) + ", stop here")
            raise RuntimeError("something is wrong with the peer list@" + str(localNode.name))

        logging.debug(str(localNode.name) + "@" + str(self.config.localaddrs[0]) + " start " + str(len(peers)) + " peers")
        filtered = []

        # Ignore peers which are still running.
        for (peer, isrunning) in self.isRunning(peers):
            if not isrunning:
                filtered += [peer]
                if peer.hasCrashed():
                    self.ui.info(str(localNode.name) + "@" + str(self.config.localaddrs[0]) + " :: starting peer %s (was crashed) ..." % peer.name)
                else:
                    self.ui.info(str(localNode.name) + "@" + str(self.config.localaddrs[0]) + " :: starting peer %s ..." % peer.name)
            else:
                self.ui.info(str(localNode.name) + "@" + str(self.config.localaddrs[0]) + " :: peer %s still running" % peer.name)

        nodes = filtered

        # Generate crash report for any crashed nodes.
        crashed = [peer for peer in peers if peer.hasCrashed()]
        self._makeCrashReports(crashed)

        # Make working directories.
        dirs = [(peer, peer.cwd()) for peer in peers]
        peers = []
        for (peer, success) in self.executor.mkdirs(dirs):
            if success:
                peers += [peer]
            else:
                self.ui.error(str(localNode.name) + "@" + str(self.config.localaddrs[0]) + " :: cannot create working directory for %s" % peer.name)
                logging.debug(str(localNode.name) + "@" + str(self.config.localaddrs[0]) + " :: cannot create working directory for %s" % peer.name)
                results.set_node_fail(peer)


        # 1. Copy broctl node.cfg configuration.
        cmds = []
        targetcfg = os.path.join(self.config.cfgdir, "node.cfg")
        for peer in peers:
            localcfg = os.path.join(self.config.cfgdir, "node.cfg_" + str(peer.name))
            # FIXME this command should better be issued in the install function
            cmds += [(peer, "mv", [localcfg, targetcfg])]

        for (peer, success, output) in self.executor.runCmdsParallel(cmds, shell=True, helper=False):
            if success:
                logging.debug(str(localNode.name) + "@" + str(self.config.localaddrs[0]) + " command successful")
            else:
                self.ui.error(str(localNode.name) + "@" + str(self.config.localaddrs[0]) + " :: cannot start %s; check output of \"diag\"" % peer.name)
                results.set_node_fail(peer)
                if output:
                    for line in output:
                        self.ui.error("%s" % line)

        # 2. Start broctl and keep connection
        cmds = []
        for peer in peers:
            cmds += [(peer, os.path.join(self.config.scriptsdir, "run-broctl"), [])]

        peers = []
        for (peer, success, output) in self.executor.runCmdsParallel(cmds, shell=True, helper=False):
            logging.debug("command to " + str(peer.name) + " success: " + str(success) + " output: " + str(output))
            if success:
                peers += [peer]
                if (output and isinstance(output, int)):
                    logging.debug(str(localNode.name) + "@" + str(self.config.localaddrs[0]) + " setting PID to " + str(output[0]))
                    peer.setPID(int(output[0]))
                else:
                    logging.debug(str(localNode.name) + "@" + str(self.config.localaddrs[0]) + " no PID value could be obtained")
            else:
                self.ui.error(str(localNode.name) + "@" + str(self.config.localaddrs[0]) + " :: cannot start %s; check output of \"diag\"" % peer.name)
                results.set_node_fail(peer)
                if output:
                    for line in output:
                        self.ui.error("%s" % line)

        # 3. Check
        # 4. Return results
        return results

    def isRunning(self, nodes, setcrashed=True):

        results = []
        cmds = []

        for node in nodes:
            pid = node.getPID()
            if not pid:
                results += [(node, False)]
                continue

            cmds += [(node, "check-pid", [str(pid)])]

        for (node, success, output) in self.executor.runHelperParallel(cmds):

            # If we cannot connect to the host at all, we filter it out because
            # the process might actually still be running but we can't tell.
            if output == None:
                if self.config.cron == "0":
                    self.ui.error(str(localNode.name) + " :: cannot connect to %s" % node.name)
                continue

            results += [(node, success)]

            if not success:
                if setcrashed:
                    # Grmpf. It crashed.
                    node.clearPID()
                    node.setCrashed()

        return results

    def waitForBros(self, nodes, status, timeout, ensurerunning):
        # If ensurerunning is true, process must still be running.
        if ensurerunning:
            running = self.isRunning(nodes)
        else:
            running = [(node, True) for node in nodes]

        results = []

        # Determine set of nodes still to check.
        todo = {}
        for (node, isrunning) in running:
            if isrunning:
                todo[node.name] = node
            else:
                results += [(node, False)]

        while True:
            # Determine  whether process is still running. We need to do this
            # before we get the state to avoid a race condition.
            running = self.isRunning(todo.values(), setcrashed=False)

            # Check nodes' .status file
            cmds = []
            for node in todo.values():
                cmds += [(node, "cat-file", ["%s/.status" % node.cwd()])]

            for (node, success, output) in self.executor.runHelperParallel(cmds):
                if success:
                    try:
                        (stat, loc) = output[0].split()
                        if status in stat:
                            # Status reached. Cool.
                            del todo[node.name]
                            results += [(node, True)]
                    except IndexError:
                        # Something's wrong. We give up on that node.
                        del todo[node.name]
                        results += [(node, False)]

            for (node, isrunning) in running:
                if node.name in todo and not isrunning:
                    # Alright, a dead node's status will not change anymore.
                    del todo[node.name]
                    results += [(node, False)]

            if not todo:
                # All done.
                break

            # Wait a bit before we start over.
            time.sleep(1)

            # Timeout reached?
            timeout -= 1
            if timeout <= 0:
                break

            logging.debug("Waiting for %d node(s)..." % len(todo))

        for node in todo.values():
            # These did time-out.
            results += [(node, False)]

        if todo:
            logging.debug("Timeout while waiting for %d node(s)" % len(todo))

        return results

    def logAction(self, node, action):
        if self.config.statslogenable == "0":
            return
        t = time.time()
        out = open(self.config.statslog, "a")
        out.write("%s %s action %s\n" % (t, node, action))
        out.close()

    # Do a "post-terminate crash" for the given nodes.
    def _makeCrashReports(self, nodes):
        for n in nodes:
            self.pluginregistry.broProcessDied(n)

        msg = "If you want to help us debug this problem, then please forward\nthis mail to reports@bro.org\n"
        cmds = []
        for node in nodes:
            cmds += [(node, "run-cmd", [os.path.join(self.config.scriptsdir, "post-terminate"), node.cwd(), "crash"])]

        for (node, success, output) in self.executor.runHelperParallel(cmds):
            if success:
                msuccess, moutput = self.sendMail("Crash report from %s" % node.name, msg + "\n".join(output))
                if not msuccess:
                    self.ui.error("error occurred while trying to send mail: %s" % moutput[0])
            else:
                self.ui.error("error running post-terminate for %s: %s" % (node.name, output[0]))

            node.clearCrashed()

    def sendMail(self, subject, body):
        if not self.config.sendmail:
            return True, ""

        cmd = "%s '%s'" % (os.path.join(self.config.scriptsdir, "send-mail"), subject)
        (success, output) = execute.runLocalCmd(cmd, "", body)
        return success, output

    # Stop Bro processes on nodes.
    def stop(self, nodes):
        results = cmdresult.CmdResult()
        manager = []
        proxies = []
        workers = []

        for n in nodes:
            n.setExpectRunning(False)

            if n.type == "worker":
                workers += [n]
            elif n.type == "proxy":
                proxies += [n]
            else:
                manager += [n]


        # Stop nodes. Do it in the order workers, proxies, manager
        # (the reverse of "start").

        self._stopNodes(workers, results)

        if results.failed():
            for n in (proxies + manager):
                results.set_node_fail(n)
            return results

        self._stopNodes(proxies, results)

        if results.failed():
            for n in manager:
                results.set_node_fail(n)
            return results

        self._stopNodes(manager, results)

        return results

    def _stopNodes(self, nodes, results):

        running = []

        # Check for crashed nodes.
        for (node, isrunning) in self.isRunning(nodes):
            if isrunning:
                running += [node]
                self.ui.info("stopping %s ..." % node.name)
            else:
                results.set_node_success(node)

                if node.hasCrashed():
                    self.ui.info("%s not running (was crashed)" % node.name)
                    self._makeCrashReports([node])
                else:
                    self.ui.info("%s not running" % node.name)

        # Helper function to stop nodes with given signal.
        def stop(nodes, signal):
            cmds = []
            for node in nodes:
                cmds += [(node, "stop", [str(node.getPID()), str(signal)])]

            return self.executor.runHelperParallel(cmds)

        # Stop nodes.
        for (node, success, output) in stop(running, 15):
            if not success:
                self.ui.error("failed to send stop signal to %s" % node.name)

        if running:
            time.sleep(1)

        # Check whether they terminated.
        terminated = []
        kill = []
        for (node, success) in self.waitForBros(running, "TERMINATED", int(self.config.stoptimeout), False):
            if not success:
                # Check whether it crashed during shutdown ...
                result = self.isRunning([node])
                for (node, isrunning) in result:
                    if isrunning:
                        self.ui.info("%s did not terminate ... killing ..." % node.name)
                        kill += [node]
                    else:
                        # crashed flag is set by isRunning().
                        self.ui.info("%s crashed during shutdown" % node.name)

        if kill:
            # Kill those which did not terminate gracefully.
            stop(kill, 9)
            # Give them a bit to disappear.
            time.sleep(5)

        # Check which are still running. We check all nodes to be on the safe side
        # and give them a bit more time to finally disappear.
        timeout = 10

        todo = {}
        for node in running:
            todo[node.name] = node

        while True:

            running = self.isRunning(todo.values(), setcrashed=False)

            for (node, isrunning) in running:
                if node.name in todo and not isrunning:
                    # Alright, it's gone.
                    del todo[node.name]
                    terminated += [node]
                    results.set_node_success(node)

            if not todo:
                # All done.
                break

            # Wait a bit before we start over.

            if timeout <= 0:
                break

            time.sleep(1)
            timeout -= 1

        for node in todo:
            results.set_node_fail(node)

        # Do post-terminate cleanup for those which terminated gracefully.
        cleanup = [node for node in terminated if not node.hasCrashed()]

        cmds = []
        for node in cleanup:
            crashflag = ""
            if node in kill:
                crashflag = "killed"

            cmds += [(node, "run-cmd", [os.path.join(self.config.scriptsdir, "post-terminate"), node.cwd(), crashflag])]

        for (node, success, output) in self.executor.runHelperParallel(cmds):
            if success:
                self.logAction(node, "stopped")
            else:
                self.ui.error("error running post-terminate for %s: %s" % (node.name, output[0]))
                self.logAction(node, "stopped (failed)")

            node.clearPID()
            node.clearCrashed()

        return results


    # Output status summary for nodes.
    def status(self, nodes):
        results = cmdresult.CmdResult()

        self.ui.info("Getting process status ...")
        all = self.isRunning(nodes)
        running = []

        cmds1 = []
        cmds2 = []
        for (node, isrunning) in all:
            if isrunning:
                running += [node]
                cmds1 += [(node, "cat-file", ["%s/.startup" % node.cwd()])]
                cmds2 += [(node, "cat-file", ["%s/.status" % node.cwd()])]

        startups = self.executor.runHelperParallel(cmds1)
        statuses = self.executor.runHelperParallel(cmds2)

        startups = dict([(n.name, success and util.fmttime(output[0]) or "???") for (n, success, output) in startups])
        statuses = dict([(n.name, success and output[0].split()[0].lower() or "???") for (n, success, output) in statuses])

        peers = {}
        nodes = [n for n in running if statuses[n.name] == "running"]

        self.ui.info("Getting peer status ...")

        for (node, success, args) in self._queryPeerStatus(nodes):
            if success:
                peers[node.name] = []
                for f in args[0].split():
                    keyval = f.split("=")
                    if len(keyval) > 1:
                        (key, val) = keyval
                        if key == "peer" and val != "":
                            peers[node.name] += [val]

        for (node, isrunning) in all:
            node_info = {
                "name": node.name,
                "type": node.type,
                "host": node.host,
                "status": "stopped",
                "pid": None,
                "peers": None,
                "started": None,
            }
            if isrunning:
                node_info["status"] = statuses[node.name]
            elif node.hasCrashed():
                node_info["status"] = "crashed"

            if isrunning:
                node_info["pid"] = node.getPID()

                if node.name in peers:
                    node_info["peers"] = len(peers[node.name])
                else:
                    node_info["peers"] = "???"

                node_info["started"] = startups[node.name]

            results.set_node_data(node, True, node_info)

        return results

    # Check the configuration for nodes without installing first.
    def checkConfigs(self, nodes):
        return self._doCheckConfig(nodes, False, False)

    # Prints the loaded_scripts.log for either the installed scripts
    # (if check argument is false), or the original scripts (if check arg is true)
    def listScripts(self, nodes, check):
        return self._doCheckConfig(nodes, not check, True)


    def _doCheckConfig(self, nodes, installed, list_scripts):
        results = cmdresult.CmdResult()
        nodeList = nodes
        manager = self.config.manager()

        all = [(node, os.path.join(self.config.tmpdir, "check-config-%s" % node.name)) for node in nodes]

        if not os.path.exists(os.path.join(self.config.scriptsdir, "broctl-config.sh")):
            self.ui.error("broctl-config.sh not found (try 'broctl install')")
            results.set_cmd_fail()
            return results

        nodes = []
        for (node, cwd) in all:
            if os.path.isdir(cwd):
                if not self.executor.rmdir(self.config.manager(), cwd):
                    self.ui.error("cannot remove directory %s on manager" % cwd)
                    results.set_cmd_fail()
                    return results

            if not self.executor.mkdir(self.config.manager(), cwd):
                self.ui.error("cannot create directory %s on manager" % cwd)
                results.set_cmd_fail()
                return results

            nodes += [(node, cwd)]

        cmds = []
        for (node, cwd) in nodes:

            env = _makeEnvParam(node)

            installed_policies = installed and "1" or "0"
            print_scripts = list_scripts and "1" or "0"

            install.makeLayout(cwd, self.ui, True)
            install.makeLocalNetworks(cwd, self.ui, True)
            install.makeConfig(cwd, self.ui, True)

            cmd = os.path.join(self.config.scriptsdir, "check-config") + " %s %s %s %s" % (installed_policies, print_scripts, cwd, " ".join(_makeBroParams(node, False)))
            cmd += " broctl/check"

            cmds += [((node, cwd), cmd, env, None)]

        # node.cfg configuration per peer in the hierarchy
        all = [(peer, os.path.join(self.config.tmpdir, "check-config-%s" % peer.name)) for peer in config.Config.nodes("peers")]
        for (peer, cwd) in all:
            install.makeNodeConfig(cwd, peer, self.ui, True)

        for ((node, cwd), success, output) in execute.runLocalCmdsParallel(cmds):
            results.set_node_output(node, success, output)
            self.executor.rmdir(manager, cwd)

        return results

    def _queryPeerStatus(self, nodes):
        running = self.isRunning(nodes)

        eventlist = []
        for (node, isrunning) in running:
            if isrunning:
                eventlist += [(node, "Control::peer_status_request", [], "Control::peer_status_response")]

        return events.sendEventsParallel(eventlist)

    def executeCmd(self, nodes, cmd):
        results = cmdresult.CmdResult()

        for node, success, out in self.executor.runShellCmdsParallel([(n, cmd) for n in nodes]):
            results.set_node_output(node, success, out)

        return results

    # Clean up the working directory for nodes (flushes state).
    # If cleantmp is true, also wipes ${tmpdir}; this is done
    # even when the node is still running.
    def cleanup(self, nodes, cleantmp=False):
        def addfailed(orig, res):
            for (n, status) in res:
                if not status:
                    orig.add(n.name)

            return orig


        results = cmdresult.CmdResult()

        result = self.isRunning(nodes)
        running    = [node for (node, on) in result if on]
        notrunning = [node for (node, on) in result if not on]

        for node in running:
            self.ui.info("   %s is still running, not cleaning work directory" % node)

        results1 = self.executor.rmdirs([(n, n.cwd()) for n in notrunning])
        results2 = self.executor.mkdirs([(n, n.cwd()) for n in notrunning])
        failed = set()
        failed = addfailed(failed, results1)
        failed = addfailed(failed, results2)

        for node in notrunning:
            node.clearCrashed()

        if cleantmp:
            results3 = self.executor.rmdirs([(n, self.config.tmpdir) for n in running + notrunning])
            results4 = self.executor.mkdirs([(n, self.config.tmpdir) for n in running + notrunning])
            failed = addfailed(failed, results3)
            failed = addfailed(failed, results4)

        for node in nodes:
            if node.name in failed:
                results.set_node_fail(node)
            else:
                results.set_node_success(node)

        return results

    # Report diagnostics for nodes (e.g., stderr output).
    def crashDiag(self, nodes):
        results = cmdresult.CmdResult()

        for node in nodes:
            if not self.executor.isdir(node, node.cwd()):
                results.set_node_output(node, False, ["No work dir found"])
                continue

            (rc, output) = self.executor.runHelper(node, "run-cmd", [os.path.join(self.config.scriptsdir, "crash-diag"), node.cwd()])
            if not rc:
                errmsgs = ["error running crash-diag for %s" % node.name]
                if output:
                    errmsgs += output
                results.set_node_output(node, False, errmsgs)
                continue

            results.set_node_output(node, True, output)

        return results

    def capstats(self, nodes, interval):
        results = cmdresult.CmdResult()

        if self.config.capstatspath:
            for (node, netif, success, vals) in self.getCapstatsOutput(nodes, interval):
                if not success:
                    vals = { "output": vals }
                results.set_node_data(node, success, vals)

        return results

    # Gather capstats from interfaces.
    #
    # Returns a list of tuples of the form (node, netif, success, vals)
    # where 'netif' is the network interface name used by capstats on
    # the 'node', and 'success' is a boolean indicating whether or not
    # we were able to get the data; in case there's no error, 'vals' maps
    # tags to their values (otherwise, 'vals' is an error message).
    #
    # Tags are those as returned by capstats on the command-line.
    #
    # If there is more than one node, then the results will also contain
    # one "pseudo-node" of the name "$total" with the sum of all individual
    # values.
    def getCapstatsOutput(self, nodes, interval):
        results = []

        hosts = {}
        for node in nodes:
            if not node.interface:
                continue

            netif = self.getCapstatsInterface(node)

            try:
                hosts[(node.addr, netif)] = node
            except AttributeError:
                continue

        cmds = []

        for (addr, interface) in hosts.keys():
            node = hosts[addr, interface]

            # Interface name needs to be quoted because the eval command
            # is used (this prevents any metacharacters in name from being
            # interpreted by the shell).
            capstats = [self.config.capstatspath, "-I", str(interval), "-n", "1", "-i", "'%s'" % interface]

            cmds += [(node, "run-cmd", capstats)]

        outputs = self.executor.runHelperParallel(cmds)

        totals = {}

        for (node, success, output) in outputs:
            netif = self.getCapstatsInterface(node)

            if not success:
                if output:
                    results += [(node, netif, False, "%s: capstats failed (%s)" % (node.name, output[0]))]
                else:
                    results += [(node, netif, False, "%s: cannot execute capstats" % node.name)]
                continue

            if not output:
                results += [(node, netif, False, "%s: no capstats output" % node.name)]
                continue

            fields = output[0].split()[1:]

            if not fields:
                results += [(node, netif, False, "%s: unexpected capstats output: %s" % (node.name, output[0]))]
                continue

            vals = {}

            try:
                for field in fields:
                    (key, val) = field.split("=")
                    val = float(val)
                    vals[key] = val

                    if key in totals:
                        totals[key] += val
                    else:
                        totals[key] = val

                results += [(node, netif, True, vals)]

            except ValueError:
                results += [(node, netif, False, "%s: unexpected capstats output: %s" % (node.name, output[0]))]

        # Add pseudo-node for totals
        if len(nodes) > 1:
            results += [(node_mod.Node(self.config, "$total"), None, True, totals)]

        return results


    def getCapstatsInterface(self, node):
        netif = node.interface

        # If PF_RING+DNA with pfdnacluster_master is being used, then this hack
        # is needed to prevent capstats from trying to use the same interface
        # name as Bro.
        if netif.startswith("dnacluster:") and netif.count("@") == 1:
            netif = netif.split("@", 1)[0]

        return netif


    # Get current statistics from cFlow.
    #
    # Returns dict of the form port->(cum-pkts, cum-bytes).
    #
    # Returns None if we can't run the helper sucessfully.
    def getCFlowStatus(self):
        (success, output) = execute.runLocalCmd(os.path.join(self.config.scriptsdir, "cflow-stats"))
        if not success or not output:
            self.ui.error("failed to run cflow-stats")
            return None

        vals = {}

        for line in output:
            try:
                (port, pps, bps, pkts, bytes) = line.split()
                vals[port] = (float(pkts), float(bytes))
            except ValueError:
                # Probably an error message because we can't connect.
                self.ui.error("failed to get cFlow statistics: %s" % line)
                return None

        return vals

    # Calculates the differences between two getCFlowStatus() calls.
    # Returns a list of tuples in the same form as getCapstatsOutput() does.
    def calculateCFlowRate(self, start, stop, interval):
        diffs = [(port, stop[port][0] - start[port][0], (stop[port][1] - start[port][1])) for port in start.keys() if port in stop]

        rates = []
        for (port, pkts, bytes) in diffs:
            vals = { "kpps" : "%.1f" % (pkts / 1e3 / interval) }
            if start[port][1] >= 0:
                vals["mbps"] = "%.1f" % (bytes * 8 / 1e6 / interval)

            rates += [(port, None, vals)]

        return rates

    # Update the configuration of a running instance on the fly.
    def update(self, nodes):
        results = cmdresult.CmdResult()

        running = self.isRunning(nodes)
        zone = self.config.zoneid
        if not zone:
            zone = "NOZONE"

        cmds = []
        for (node, isrunning) in running:
            if isrunning:
                env = _makeEnvParam(node)
                env += " BRO_DNS_FAKE=1"
                args = " ".join(_makeBroParams(node, False))
                cmds += [(node.name, os.path.join(self.config.scriptsdir, "update") + " %s %s %s/tcp %s" % (util.formatBroAddr(node.addr), zone, node.getPort(), args), env, None)]
                self.ui.info("updating %s ..." % node.name)

        res = execute.runLocalCmdsParallel(cmds)

        for (tag, success, output) in res:
            node = self.config.nodes(tag=tag)[0]
            if not success:
                self.ui.error("could not update %s: %s" % (tag, output[0]))
                results.set_node_fail(node)
            else:
                self.ui.info("%s: %s" % (tag, output[0]))
                results.set_node_success(node)

        return results

    # Gets disk space on all volumes relevant to broctl installation.
    # Returns a list of the form:  [ (host, diskinfo), ...]
    # where diskinfo is a list of the form DiskInfo named tuple objects (fs, total, used, avail, percent) or
    # ["FAIL", <error message>] if an error is encountered.
    def df(self, nodes):
        results = cmdresult.CmdResult()

        DiskInfo = namedtuple("DiskInfo", ("fs", "total", "used", "available", "percent"))
        dirs = ("logdir", "bindir", "helperdir", "cfgdir", "spooldir", "policydir", "libdir", "tmpdir", "staticdir", "scriptsdir")

        df = {}
        for node in nodes:
            df[node.name] = {}

        for dir in dirs:
            path = self.config.config[dir]

            cmds = []
            for node in nodes:
                if dir == "logdir" and node.type != "manager":
                    # Don't need this on the workers/proxies.
                    continue

                cmds += [(node, "df", [path])]

            res = self.executor.runHelperParallel(cmds)

            for (node, success, output) in res:
                if success:
                    if not output:
                        df[node.name]["FAIL"] = "no output from df helper"
                        continue

                    fields = output[0].split()

                    fs = fields[0]
                    # Ignore NFS mounted volumes.
                    if ":" in fs:
                        continue

                    total = float(fields[1])
                    used = float(fields[2])
                    avail = float(fields[3])
                    perc = used * 100.0 / (used + avail)
                    df[node.name][fs] = DiskInfo(fs, total, used, avail, perc)
                else:
                    if output:
                        msg = output[0]
                    else:
                        msg = "unknown failure"
                    df[node.name]["FAIL"] = msg

        for node in nodes:
            success = "FAIL" not in df[node.name]
            results.set_node_data(node, success, df[node.name])

        return results

    # Returns a list of tuples of the form (node, error, vals) where 'error' is
    # an error message string, or None if there was no error.  'vals' is a list
    # of dicts which map tags to their values.  Tags are "pid", "proc", "vsize",
    # "rss", "cpu", and "cmd".
    def getTopOutput(self, nodes):

        results = []
        cmds = []

        running = self.isRunning(nodes)

        # Get all the PIDs first.

        pids = {}
        parents = {}

        for (node, isrunning) in running:
            if isrunning:
                pid = node.getPID()
                pids[node.name] = [pid]
                parents[node.name] = str(pid)

                cmds += [(node, "get-childs", [str(pid)])]
            else:
                results += [(node, "not running", [{}])]
                continue

        if not cmds:
            return results

        for (node, success, output) in self.executor.runHelperParallel(cmds):

            if not success:
                results += [(node, "cannot get child pids", [{}])]
                continue

            pids[node.name] += [int(line) for line in output]

        cmds = []
        hosts = {}

        # Now run top once per host.
        for node in nodes:   # Do the loop again to keep the order.
            if node.name not in pids:
                continue

            if node.host in hosts:
                continue

            hosts[node.host] = 1

            cmds += [(node, "top", [])]

        if not cmds:
            return results

        res = {}
        for (node, success, output) in self.executor.runHelperParallel(cmds):
            res[node.host] = (success, output)

        # Gather results for all the nodes that are running
        for node in nodes:
            if node.name not in pids:
                continue

            success, output = res[node.host]

            if not success or not output:
                results += [(node, "cannot get top output", [{}])]
                continue

            procs = [line.split() for line in output if int(line.split()[0]) in pids[node.name]]

            if not procs:
                # It's possible that the process is no longer there.
                results += [(node, "not running", [{}])]
                continue

            vals = []

            try:
                for p in procs:
                    d = {}
                    d["pid"] = int(p[0])
                    d["proc"] = (p[0] == parents[node.name] and "parent" or "child")
                    d["vsize"] = int(float(p[1])) #May be something like 2.17684e+9
                    d["rss"] = int(float(p[2]))
                    d["cpu"] = p[3]
                    d["cmd"] = " ".join(p[4:])
                    vals += [d]
            except ValueError as err:
                results += [(node, "unexpected top output: %s" % err, [{}])]
                continue

            results += [(node, None, vals)]

        return results

    # Produce a top-like output for node's processes.
    def top(self, nodes):
        results = cmdresult.CmdResult()

        for (node, error, vals) in self.getTopOutput(nodes):
            top_info = { "name" : node.name, "type" : node.type,
                         "host" : node.host, "pid" : None, "proc" : None,
                         "vsize" : None, "rss" : None, "cpu" : None,
                         "cmd" : None, "error" : None }
            if error:
                top_info["error"] = error
                results.set_node_data(node, False, { "procs": [top_info] })
                continue

            proclist = []
            for d in vals:
                top_info2 = top_info.copy()
                top_info2.update(d)
                proclist.append(top_info2)

            results.set_node_data(node, True, { "procs": proclist })

        return results

    def printID(self, nodes, id):
        running = self.isRunning(nodes)

        eventlist = []
        for (node, isrunning) in running:
            if isrunning:
                eventlist += [(node, "Control::id_value_request", [id], "Control::id_value_response")]

        results = cmdresult.CmdResult()
        for (node, success, args) in events.sendEventsParallel(eventlist):
            results.set_node_output(node, success, args)

        return results


    def _queryNetStats(self, nodes):
        running = self.isRunning(nodes)

        eventlist = []
        for (node, isrunning) in running:
            if isrunning:
                eventlist += [(node, "Control::net_stats_request", [], "Control::net_stats_response")]

        return events.sendEventsParallel(eventlist)

    def peerStatus(self, nodes):
        results = cmdresult.CmdResult()
        for (node, success, args) in self._queryPeerStatus(nodes):
            if success:
                out = args[0]
            else:
                out = args
            results.set_node_output(node, success, out)

        return results

    def netStats(self, nodes):
        results = cmdresult.CmdResult()
        for (node, success, args) in self._queryNetStats(nodes):
            if success:
                out = args[0].strip()
            else:
                out = args
            results.set_node_output(node, success, out)

        return results

    def processTrace(self, trace, bro_options, bro_scripts):
        results = cmdresult.CmdResult()

        if not os.path.isfile(trace):
            self.ui.error("trace file not found: %s" % trace)
            results.set_cmd_fail()
            return results

        if not os.path.exists(os.path.join(self.config.scriptsdir, "broctl-config.sh")):
            self.ui.error("broctl-config.sh not found (try 'broctl install')")
            results.set_cmd_fail()
            return results

        standalone = (self.config.standalone == "1")
        if standalone:
            tag = "standalone"
        else:
            tag = "workers"

        node = self.config.nodes(tag=tag)[0]

        cwd = os.path.join(self.config.tmpdir, "testing")

        if not self.executor.rmdir(self.config.manager(), cwd):
            self.ui.error("cannot remove directory %s on manager" % cwd)
            results.set_cmd_fail()
            return results

        if not self.executor.mkdir(self.config.manager(), cwd):
            self.ui.error("cannot create directory %s on manager" % cwd)
            results.set_cmd_fail()
            return results

        env = _makeEnvParam(node)

        bro_args =  " ".join(bro_options + _makeBroParams(node, False))
        bro_args += " broctl/process-trace"

        if bro_scripts:
            bro_args += " " + " ".join(bro_scripts)

        cmd = os.path.join(self.config.scriptsdir, "run-bro-on-trace") + " %s %s %s %s" % (0, cwd, trace, bro_args)

        self.ui.info(cmd)

        (success, output) = execute.runLocalCmd(cmd, env, donotcaptureoutput=True)

        if not success:
            results.set_cmd_fail()

        for line in output:
            self.ui.info(line)

        self.ui.info("\n### Bro output in %s" % cwd)

        return results

    def install(self, local_only):
        results = cmdresult.CmdResult()

        localNode = self.config.getLocalNode()
        logging.debug(str(localNode.name) + "@" + str(self.config.localaddrs[0]) + " :: *** Install nodes")

        if not self.config.recordBroVersion():
            results.set_cmd_fail()
            return results

        manager = self.config.manager()

        # Delete previously installed policy files to not mix things up.
        policies = [self.config.policydirsiteinstall, self.config.policydirsiteinstallauto]

        for p in policies:
            if os.path.isdir(p):
                self.ui.info("removing old policies in %s ..." % p)
                if not self.executor.rmdir(manager, p):
                    results.set_cmd_fail()

        self.ui.info("creating policy directories ...")
        for p in policies:
            if not self.executor.mkdir(manager, p):
                results.set_cmd_fail()

        # Install local site policy.
        if self.config.sitepolicypath:
            self.ui.info("installing site policies ...")
            dst = self.config.policydirsiteinstall
            for dir in self.config.sitepolicypath.split(":"):
                dir = self.config.subst(dir)
                for file in glob.glob(os.path.join(dir, "*")):
                    if not execute.install(file, dst, self.ui):
                        results.set_cmd_fail()

        install.makeLayout(self.config.policydirsiteinstallauto, self.ui)
        if not install.makeLocalNetworks(self.config.policydirsiteinstallauto, self.ui):
            results.set_cmd_fail()
        install.makeConfig(self.config.policydirsiteinstallauto, self.ui)

        current = self.config.subst(os.path.join(self.config.logdir, "current"))
        try:
            util.force_symlink(manager.cwd(), current)
        except (IOError, OSError) as e:
            results.set_cmd_fail()
            self.ui.error("failed to update current log symlink")
            return results

        if not install.generateDynamicVariableScript(self.ui):
            results.set_cmd_fail()
            return results

        if local_only:
            return results

        # Install node configuration for the overlay hierarchy on peers
        peers = self.config.nodes("peers")
        install.makeNodeConfigs(self.config.cfgdir, peers, self.ui)

        # Make sure we install each remote host only once.
        nodes = self.config.hosts(nolocal=True)
        if localNode in nodes:
            logging.debug("Bad things are happening during install")
            raise RuntimeError("Nodelist should not contain the local node instance")

        # Sync to clients.
        self.ui.info("updating nodes ...")

        if self.config.havenfs != "1":
            # Non-NFS, need to explicitly synchronize.
            dirs = []
            syncs = install.get_syncs()
            for dir in [self.config.subst(dir) for (dir, mirror) in syncs if not mirror]:
                dirs += [(n, dir) for n in nodes]

            for (node, success) in self.executor.mkdirs(dirs):
                if not success:
                    self.ui.error("cannot create directory %s on %s" % (dir, node.name))
                    results.set_cmd_fail()

            # An error at this point means the entire install failed.
            if results.failed():
                return results

            paths = [os.path.normpath(self.config.subst(dir)) for (dir, mirror) in syncs if mirror]
            if not execute.sync(nodes, paths, self.ui):
                results.set_cmd_fail()
                return results

        else:
            # NFS. We only need to take care of the spool/log directories.
            paths = [self.config.spooldir]
            paths += [self.config.tmpdir]

            dirs = []
            syncs = install.get_nfssyncs()
            for dir in paths:
                dirs += [(n, dir) for n in nodes]

            for dir in [self.config.subst(dir) for (dir, mirror) in syncs if not mirror]:
                dirs += [(n, dir) for n in nodes]

            # We need this only on the manager.
            dirs += [(manager, self.config.logdir)]

            for (node, success) in self.executor.mkdirs(dirs):
                if not success:
                    self.ui.error("cannot create (some of the) directories %s on %s" % (",".join(paths), node.name))
                    results.set_cmd_fail()

            # An error at this point means the entire install failed.
            if results.failed():
                return results

            paths = [self.config.subst(dir) for (dir, mirror) in syncs if mirror]
            if not execute.sync(nodes, paths, self.ui):
                results.set_cmd_fail()
                return results

        # Save current node configuration state.
        self.config.updateNodeCfgHash()

        # Save current configuration state.
        self.config.updateBroctlCfgHash()

        return results


    # Triggers all activity which is to be done regularly via cron.
    def doCron(self, watch):
        if not self.config.hasAttr("cronenabled"):
            self.config._setState("cronenabled", True)
        if not self.config.cronenabled:
            return

        if not os.path.exists(os.path.join(self.config.scriptsdir, "broctl-config.sh")):
            self.ui.error("broctl-config.sh not found (try 'broctl install')")
            return

        # Flag to indicate that we're running from cron.
        self.config.config["cron"] = "1"

        cronui = cron.CronUI()
        tasks = cron.CronTasks(cronui, self.config, self)

        cronui.bufferOutput()

        if watch:
            # Check if node state matches expected state, and start/stop if
            # necessary.
            startlist = []
            stoplist = []
            for (node, isrunning) in self.isRunning(self.config.nodes()):
                expectrunning = node.getExpectRunning()

                if not isrunning and expectrunning:
                    startlist.append(node)
                elif isrunning and not expectrunning:
                    stoplist.append(node)

            if startlist:
                results = self.start(startlist)
            if stoplist:
                results = self.stop(stoplist)

        # Check for dead hosts.
        tasks.checkHosts()

        # Generate statistics.
        tasks.logStats(5)

        # Check available disk space.
        tasks.checkDiskSpace()

        # Expire old log files.
        tasks.expireLogs()

        # Update the HTTP stats directory.
        tasks.updateHTTPStats()

        # Run external command if we have one.
        tasks.runCronCmd()

        # Mail potential output.
        output = cronui.getBufferedOutput()
        if output:
            success, out = self.sendMail("cron: " + output.splitlines()[0], output)
            if not success:
                self.ui.error("error occurred while trying to send mail: %s" % out[0])

        self.config.config["cron"] = "0"
        logging.debug("cron done")

